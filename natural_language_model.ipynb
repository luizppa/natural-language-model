{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74df6502",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "The Word2Vec technique is a method for obtaining and word embeding suitable for natural language processing, such as finding synonyms, making analogies and suggesting missing words for a sentence. It uses a neural network to learn from a large text (or corpus) am build a n-dimensional space containing one vector for each word on the training vocabulary.\n",
    "\n",
    "This technique is relatively recent, being first published by a team of Google researches in 2013. The two main algorithms applied are skipgram and cbow. In both cases, we use a set of hyperparams, such as number of features in the word embeding (or the dimensionality of each word vector), the number of epochs and the size of our context window, to define the way our language model will be trained.\n",
    "\n",
    "The objective of this notebook, is to explore how the variation of these hyperparameters will affect the accuracy of our model. The following implementation was made using gensim. It covers the training and validation of various models, however, **it does not cover any kind of sophisticated text lemmatization**, since we are using an already noralized corpus. Needless to say, that is not the real word scenario in most cases, therefore, any pratical implementation of Word2Vec should take this into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f51cd77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/eem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from time import time, sleep\n",
    "import multiprocessing\n",
    "\n",
    "# File management\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "from os import path, remove\n",
    "\n",
    "# NLP\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.test.utils import datapath\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Data analisis\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b47162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(done = 0, total = 100):\n",
    "    progress = 20*(done/total)\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(\"[%-20s] %d%%\" % ('='*math.floor(progress), 5*progress))\n",
    "    sys.stdout.flush()\n",
    "    if done == total:\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f918060",
   "metadata": {},
   "source": [
    "## Downloading the corpus\n",
    "\n",
    "The first thing we got to do is to download the corpus we will be using to train our model. Here we will use a text of lowercase and unponctuated englih words supported by gensim as standard. The file is 100MB long, so it wont be included on the repository, nevertheless, the code bellow should perform the propper configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1378245f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic institutions anarchists advocate \n"
     ]
    }
   ],
   "source": [
    "corpus_uri = 'http://mattmahoney.net/dc/text8.zip'\n",
    "target_name = './corpus.txt'\n",
    "corpus_language = 'english'\n",
    "\n",
    "if not path.exists(target_name):\n",
    "    try:\n",
    "        resp = urlopen(corpus_uri)\n",
    "        file = ZipFile(BytesIO(resp.read()))\n",
    "\n",
    "        target_file = open(target_name, 'w')\n",
    "        for line in file.open(file.namelist()[0]).readlines():\n",
    "            target_file.write(line.decode('utf-8'))\n",
    "        target_file.close()\n",
    "    except:\n",
    "        if path.exists(target_name):\n",
    "            remove(target_name)\n",
    "            \n",
    "corpus = open(target_name, 'r')\n",
    "content = corpus.read()\n",
    "print(content[:1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47823457",
   "metadata": {},
   "source": [
    "## Pre processing\n",
    "\n",
    "Our text is mostly normalized, so all we are going to do is break the text into sentences (using gensim standard support) and remove stopwords using the natural language toolkit library group of english stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573d5f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into sentences\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "sentences = list(itertools.islice(Text8Corpus(target_name),None))\n",
    "sentences = [[word for word in sentence if word not in stopwords] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012e60a0",
   "metadata": {},
   "source": [
    "## Creating and training the language model\n",
    "\n",
    "The following functions are used for building the vocabulary and training our models. We will have a set of 81 combinations of parametres for each algorithm, therefore we will have to train 162 models. Each model takes a few minutes to train so it would be a very time consuming process to run each time. Gladly, gensim allows us to save our models into files, so we only have to train the models once.\n",
    "\n",
    "The code will then verify if for a given set of hyperparameters a model was already created, if so, it will simply load the model from its file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75cc717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(model, sentences):\n",
    "    t = time()\n",
    "    model.build_vocab(sentences, progress_per=10000)\n",
    "    return time() - t\n",
    "\n",
    "def train(model, sentences, epochs, corpus_size):\n",
    "    t = time()\n",
    "    model.train(sentences, total_examples=corpus_size, epochs=epochs, report_delay=1)\n",
    "    return time() - t\n",
    "\n",
    "def build_model(sentences, min_count, window, vector_size, alpha, epochs, sg, corpus_size, model_name):\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    model = Word2Vec(min_count=min_count, window=window, vector_size=vector_size, alpha=0.001, workers=cores-1, sg=sg)\n",
    "    corpus_size = model.corpus_count if corpus_size == 0 else corpus_size\n",
    "    \n",
    "    t = 0\n",
    "    t = t + build_vocabulary(model, sentences)\n",
    "    t = t + train(model, sentences, epochs, corpus_size)\n",
    "    \n",
    "    model.save(model_name)\n",
    "    return t\n",
    "    \n",
    "def get_model_name(sg, window, vector_size, epochs, corpus_size, min_count):\n",
    "    return \"./models/{sg}-{window}-{vector_size}-{epochs}-{corpus_size}-{min_count}.model\".format(\n",
    "        sg = 'skipgram' if sg == 1 else 'cbow',\n",
    "        window = window,\n",
    "        vector_size = vector_size,\n",
    "        epochs = epochs,\n",
    "        corpus_size = corpus_size,\n",
    "        min_count = min_count\n",
    "    )\n",
    "    \n",
    "def build_if_not_exists(sentences, sg=1, window=2, vector_size=100, epochs=30, corpus_size=0, min_count=50):\n",
    "    model_name = get_model_name(sg, window, vector_size, epochs, corpus_size, min_count)\n",
    "    \n",
    "    if not path.isfile(model_name):\n",
    "        return build_model(\n",
    "            sentences = sentences,\n",
    "            min_count = min_count,\n",
    "            window = window,\n",
    "            vector_size = vector_size,\n",
    "            alpha = 0.001,\n",
    "            epochs = epochs,\n",
    "            sg = sg,\n",
    "            corpus_size = corpus_size,\n",
    "            model_name = model_name\n",
    "        )\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def get_model(corpus_size, sg=1, window=2, vector_size=100, epochs=30, min_count=50):\n",
    "    model_name = get_model_name(sg, window, vector_size, epochs, corpus_size, min_count)\n",
    "    \n",
    "    if path.isfile(model_name):\n",
    "        return Word2Vec.load(model_name).wv\n",
    "    else:\n",
    "        print('Model not trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ec2d64",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "For the hyperparameters to be tunned we will have:\n",
    "\n",
    "1. vector_sizes, wich indicates the number of features in each word embeding of the model\n",
    "2. windows, which reffers to the size of the context use to evaluate the sentences during training\n",
    "3. corpus_sizes, which is the number of sentences from our corpus that will be considered for training\n",
    "4. the number of epochs performed during training\n",
    "\n",
    "each one of the parameters will have three values and each value will be evenly permutated generating 81 combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f444c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_sizes = [50, 300]\n",
    "windows = [2, 5, 10]\n",
    "min_counts = [10, 30, 50]\n",
    "corpus_sizes = [math.floor(len(sentences)*.5), len(sentences)]\n",
    "epochs = [10, 20, 30]\n",
    "\n",
    "params = [list(i) for i in itertools.product(windows, vector_sizes, epochs, corpus_sizes, min_counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "804c696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_params(sentences, algorithm, params):\n",
    "    done = 0\n",
    "    total = len(params)\n",
    "    t = 0\n",
    "    algorithm_name = 'skipgram' if algorithm == 1 else 'cbow'\n",
    "    print('Training {} models:'.format(algorithm_name))\n",
    "    progress_bar(done, total)\n",
    "\n",
    "    for param in params:\n",
    "        t = t + build_if_not_exists(sentences, algorithm, param[0], param[1], param[2], param[3], param[4])\n",
    "        done = done + 1\n",
    "        progress_bar(done, total)\n",
    "\n",
    "    minutes = math.floor(t/60)\n",
    "    hours = math.floor(minutes/60)\n",
    "    minutes = minutes % 60\n",
    "    print('Took {} hours and {} minutes to make {} {} models'.format(hours, minutes, done, algorithm_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb860159",
   "metadata": {},
   "source": [
    "### Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1aacde3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training skipgram models:\n",
      "[====================] 100%\n",
      "Took 0 hours and 1 minutes to make 1 skipgram models\n"
     ]
    }
   ],
   "source": [
    "train_with_params(sentences, 1, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d51d234",
   "metadata": {},
   "source": [
    "### Cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f243d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_with_params(sentences, 0, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25319f50",
   "metadata": {},
   "source": [
    "## Analogies\n",
    "\n",
    "With our model trained, we can query for analog words. Take for example the pair germany and berlin, there is a clear relation between them, so if we were to provide a third word, it is to be expected that the model will return a fourth word bearing the same relation with the third word as the second did to the first, in this case, supose we input the word france, a reasonable answer would be paris, since paris is to france as berlin is to germany.\n",
    "\n",
    "This is shown bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87e308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(model, word, is_to, as_word):\n",
    "    result = model.most_similar(negative=[word], positive=[is_to, as_word])\n",
    "    return result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e6eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(corpus_size=len(sentences), sg=1, window=5, vector_size=100, epochs=20, min_count=30)\n",
    "print('germany is to berlin as france is to: {}'.format(analogy(model, 'germany', 'berlin', 'france')))\n",
    "print('star is to sun as planet is to: {}'.format(analogy(model, 'star', 'sun', 'planet')))\n",
    "print('man is to king as woman is to: {}'.format(analogy(model, 'man', 'king', 'woman')))\n",
    "print('teacher is to school as nurse is to: {}'.format(analogy(model, 'teacher', 'school', 'nurse')))\n",
    "print('frederick is to king as elizabeth is to: {}'.format(analogy(model, 'frederick', 'king', 'elizabeth')))\n",
    "# print('feline is to cat as canine is to: {}'.format(analogy(model, 'feline', 'cat', 'canine')))\n",
    "print('car is to road as boat is to: {}'.format(analogy(model, 'car', 'road', 'boat')))\n",
    "print('fast is to faster as easy is to: {}'.format(analogy(model, 'fast', 'faster', 'easy')))\n",
    "print('small is to big as good is to: {}'.format(analogy(model, 'small', 'big', 'good')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b2f376",
   "metadata": {},
   "source": [
    "Of course, it does not always work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e2051",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hat is to head as shirt is to: {}'.format(analogy(model, 'hat', 'head', 'shirt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0632e076",
   "metadata": {},
   "source": [
    "We can use this caracteristic to measure the accuracy of our model. Given a collection of pre stablished analogies, we will provide the first three words and search the model's reponse for the fourth. Keep in mind that the output of the model is not a single word as the exemples above, it is an array of probabilities, and each component of the array will be accounted for during the accuracy measure, this will also be done by gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d11e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download test analogies\n",
    "analogies_uri = 'https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt'\n",
    "analogies_file_name = 'questions-words.txt'\n",
    "\n",
    "if not path.exists(analogies_file_name):\n",
    "    try:\n",
    "        resp = urlopen(analogies_uri)\n",
    "        file = open(analogies_file_name, 'wb')\n",
    "        file.write(resp.read())\n",
    "        file.close()\n",
    "    except:\n",
    "        if path.exists(analogies_file_name):\n",
    "            remove(analogies_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e23827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(sg, window, vector_size, epochs, corpus_size, min_count):\n",
    "    model = get_model(corpus_size, sg, window, vector_size, epochs, min_count)\n",
    "    analogy_scores = model.evaluate_word_analogies(datapath(analogies_file_name))\n",
    "    return analogy_scores[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432803c",
   "metadata": {},
   "source": [
    "The following script will be renspondible for performing the test for each model and saving the results to a dataframe (it even has a nice progress bar). This also takes quite a lot of time, so we will save the dataframes to a csv file so we can load it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90bf620",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file_name = 'accuracy.csv'\n",
    "accuracy_df = pd.DataFrame(columns=['algorithm', 'window', 'vector size', 'epochs', 'corpus size', 'min count', 'accuracy'])\n",
    "\n",
    "if not path.exists(results_file_name):\n",
    "    total = len(params)\n",
    "    \n",
    "    # Skipgram\n",
    "    done = 0\n",
    "    print('Skipgram test progress:')\n",
    "    progress_bar(done, total)\n",
    "    for param in params:\n",
    "        accuracy = test_model(sg=1, window=param[0], vector_size=param[1], epochs=param[2], corpus_size=param[3], min_count=param[4])\n",
    "        data = {\n",
    "            'algorithm': 'skipgram',\n",
    "            'window': param[0],\n",
    "            'vector size': param[1],\n",
    "            'epochs': param[2],\n",
    "            'corpus size': param[3],\n",
    "            'min count': param[4],\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "        accuracy_df = accuracy_df.append(data, ignore_index=True)\n",
    "        done = done + 1\n",
    "        progress_bar(done, total)\n",
    "    print('')\n",
    "\n",
    "    # Cbow\n",
    "    done = 0\n",
    "    print('Cbow test progress:')\n",
    "    progress_bar(done, total)\n",
    "    for param in params:\n",
    "        accuracy = test_model(sg=0, window=param[0], vector_size=param[1], epochs=param[2], corpus_size=param[3], min_count=param[4])\n",
    "        data = {\n",
    "            'algorithm': 'cbow',\n",
    "            'window': param[0],\n",
    "            'vector size': param[1],\n",
    "            'epochs': param[2],\n",
    "            'corpus size': param[3],\n",
    "            'min count': param[4],\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "        accuracy_df = accuracy_df.append(data, ignore_index=True)\n",
    "        done = done + 1\n",
    "        progress_bar(done, total)\n",
    "        \n",
    "    # Save results\n",
    "    results_file = open(results_file_name, 'w')\n",
    "    results_file.write(accuracy_df.to_csv())\n",
    "    results_file.close()\n",
    "else:\n",
    "    accuracy_df = pd.read_csv(results_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de17b6a",
   "metadata": {},
   "source": [
    "You can see below the best achieved results for each algorithm. Skipgram had a maximum of 24.5% of accuracy rate while cbow only achieved about 8% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum skipgram accuracy: {}'.format(accuracy_df[accuracy_df.algorithm == 'skipgram'].accuracy.max()))\n",
    "print('Maximum cbow accuracy: {}'.format(accuracy_df[accuracy_df.algorithm == 'cbow'].accuracy.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a16559",
   "metadata": {},
   "source": [
    "## Hyperparameter study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62112c",
   "metadata": {},
   "source": [
    "With our accuracy data gathered, we can visualize how each paremeter affected the average accuracy obtained on the figures below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca4f21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_influence(param, dataframe):\n",
    "    skipgram_data = dataframe[dataframe.algorithm == 'skipgram'].groupby(param).mean()\n",
    "    cbow_data = dataframe[dataframe.algorithm == 'cbow'].groupby(param).mean()\n",
    "    \n",
    "    plt.title('Influence of {} in model accuracy'.format(param))\n",
    "    plt.xlabel(param)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.plot(cbow_data, label='cbow')\n",
    "    plt.scatter(x=dataframe[param].unique(), y=cbow_data)\n",
    "    plt.plot(skipgram_data, label='skipgram', color='red')\n",
    "    plt.scatter(x=dataframe[param].unique(), y=skipgram_data, color='red')\n",
    "    plt.legend()\n",
    "    \n",
    "def parameter_variance(params, dataframe):\n",
    "    data = pd.DataFrame(columns=['param', 'variance'])\n",
    "    \n",
    "    plt.title('Variance of accuracy for each parameter')\n",
    "    plt.ylabel('accuracy')\n",
    "    for param in params:\n",
    "        variance = dataframe.groupby(param).mean().var()[0]\n",
    "        data = data.append({'param': param, 'variance': variance}, ignore_index=True)\n",
    "        \n",
    "    plt.bar(range(len(params)), data['variance'], tick_label=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a09b0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=100)\n",
    "params = ['epochs', 'window', 'vector size', 'corpus size']\n",
    "\n",
    "for i in range(len(params)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plot_influence(params[i], accuracy_df)\n",
    "\n",
    "plt.subplots_adjust(hspace = .4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e752f",
   "metadata": {},
   "source": [
    "As we can see, for both algorithms the number of the epochs and the size of the context window had a much greater impact in overall accuracy, while vector size and corpus size created a smaller variation on the results. We can comprove this tendency with the following plot of the variance for each parameter.\n",
    "\n",
    "Moreover, as the vector size increases, the average accuracy decreases for the cbow algorithm. This could be an indicator of overfitting, as we try to extreact a large number of features from a limited corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ed956",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "parameter_variance(params, accuracy_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37deae90",
   "metadata": {},
   "source": [
    "With this data, we can prioritize the variation of the most relevant params and set a constant value for the remaining ones.\n",
    "\n",
    "For the skipgram algorithm, we will use a vector size of 100 and our entire corpus while using values between 30 and 70 for the min count, 20 and 60 for the number of epochs and 5 and 10 for window sizes. We will do the same for the cbow algorithm, except we will use 50 for vector size instead of 100, since it gave us a beter accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7191931",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_sizes = [100]\n",
    "windows = [5, 10]\n",
    "min_counts = [30, 50, 70]\n",
    "corpus_sizes = [len(sentences)]\n",
    "epochs = [20, 30, 60]\n",
    "skipgram_params = [list(i) for i in itertools.product(windows, vector_sizes, epochs, corpus_sizes, min_counts)]\n",
    "\n",
    "train_with_params(sentences, 1, skipgram_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c63ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_sizes = [50]\n",
    "windows = [5, 10]\n",
    "min_counts = [30, 50, 70]\n",
    "corpus_sizes = [len(sentences)]\n",
    "epochs = [20, 30, 60]\n",
    "cbow_params = [list(i) for i in itertools.product(windows, vector_sizes, epochs, corpus_sizes, min_counts)]\n",
    "\n",
    "train_with_params(sentences, 0, cbow_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7e8da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
